{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab60f84-39b3-4bdd-ae83-6527acb315e5",
   "metadata": {},
   "source": [
    "# RAG Bootcamp ◦ February 2024 ◦ Vector Institute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2276edc2-74ce-4d85-bdaa-0607c74fc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Venue: RAG Bootcamp - Vector Institute Canada\n",
    "# Talk: RAG Bootcamp: Intro to RAG with the LlamaIndexFramework\n",
    "# Speaker: Andrei Fajardo\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee89b8-a04d-4326-9392-b9e7e1bcb8af",
   "metadata": {},
   "source": [
    "![Title Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/title.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d38b38-ea48-4012-81ae-84e1d1f40a69",
   "metadata": {},
   "source": [
    "![Title Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/framework.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1f8e7-f978-4f19-bdfb-37c0d235b5bf",
   "metadata": {},
   "source": [
    "#### Notebook Setup & Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f227a52a-a147-4e8f-b7d3-e03f983fd5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-index-vector-stores-qdrant -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc383fc-19b2-47b5-af61-e83210ea9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa0f82d-bbe0-4487-a9f9-38fa8475edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-28 13:05:08--  https://arxiv.org/pdf/2402.09353.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 604050 (590K) [application/pdf]\n",
      "Saving to: ‘./data/dorav1.pdf’\n",
      "\n",
      "./data/dorav1.pdf   100%[===================>] 589.89K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-02-28 13:05:08 (5.23 MB/s) - ‘./data/dorav1.pdf’ saved [604050/604050]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget \"https://arxiv.org/pdf/2402.09353.pdf\" -O \"./data/dorav1.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c00f1-e358-498a-88c3-8e810a5a2546",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "![Motivation Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/motivation.excalidraw.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d4ce76-8eea-44cb-aa99-94844dfed9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query an LLM and ask it about DoRA\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "response = llm.complete(\"What is DoRA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f18489-4f25-40ce-86e9-697ddea7d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without specific context, it's hard to determine which \"DoRA\" you're referring to as it could mean different things in different fields. However, in the context of education, DoRA often stands for \"Division of Research Administration\" which is responsible for the oversight of research activities in an institution. Please provide more context.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0ef8d-d55c-4b64-887b-18d343503a76",
   "metadata": {},
   "source": [
    "## Basic RAG in 3 Steps\n",
    "\n",
    "![Divider Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/subheading.excalidraw.svg)\n",
    "\n",
    "\n",
    "1. Build external knowledge (i.e., uploading updated data sources)\n",
    "2. Retrieve\n",
    "3. Augment and Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a5257-20ae-468e-85d6-d4e8c46b8cb5",
   "metadata": {},
   "source": [
    "## 1. Build External Knowledge\n",
    "\n",
    "![Divider Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/step1.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2963f90-9da5-4a0d-8dbe-f16fcb8627a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the data.\n",
    "\n",
    "With llama-index, before any transformations are applied,\n",
    "data is loaded in the `Document` abstraction, which is\n",
    "a container that holds the text of the document.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "loader = SimpleDirectoryReader(input_dir=\"./data\")\n",
    "documents = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da321e2c-8428-4c04-abf2-b204416e816f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DoRA: Weight-Decomposed Low-Rank Adaptation\\nShih-Yang Liu* 1 2Chien-Yi Wang1Hongxu Yin1Pavlo Molchanov1Yu-Chiang Frank Wang1\\nKwang-Ting Cheng2Min-Hung Chen1\\nAbstract\\nAmong the widely used parameter-efficient fine-\\ntuning (PEFT) methods, LoRA and its variants\\nhave gained considerable popularity because of\\navoiding additional inference costs. However,\\nthere still often exists an accuracy gap between\\nthese methods and full fine-tuning (FT). In this\\nwork, we first introduce a novel weight decom-\\nposition analysis to investigate the inherent dif-\\nferences between FT and LoRA. Aiming to re-\\nsemble the learning capacity of FT from the\\nfindings, we propose Weight- Decomposed L ow-\\nRankAdaptation ( DoRA ). DoRA decomposes\\nthe pre-trained weight into two components, mag-\\nnitude anddirection , for fine-tuning, specifically\\nemploying LoRA for directional updates to effi-\\nciently minimize the number of trainable param-\\neters. By employing DoRA, we enhance both\\nthe learning capacity and training stability of\\nLoRA while avoiding any additional inference\\noverhead. DoRA consistently outperforms LoRA\\non fine-tuning LLaMA, LLaV A, and VL-BART\\non various downstream tasks, such as common-\\nsense reasoning, visual instruction tuning, and\\nimage/video-text understanding. We will release\\nthe code and models upon acceptance.\\n1. Introduction\\nModels that are pre-trained with extensive general domain\\ndatasets have demonstrated remarkable generalization abil-\\nities, significantly benefiting a wide array of applications,\\nfrom natural language processing (NLP) tasks (Qin et al.,\\n2023; Taori et al., 2023) to multi-modal tasks (Li et al., 2022;\\nLiu et al., 2023a). To tailor these general models for spe-\\ncific downstream tasks, full fine-tuning (FT) is commonly\\nemployed, involving the retraining of all model parameters.\\nNevertheless, as the size of models and datasets expand\\n*Work done during an internship at NVIDIA\\n1NVIDIA2HKUST. Correspondence to: Min-Hung Chen\\n<minghungc@nvidia.com >.\\nMagnitude\\nB\\nAPretrained\\nWeight\\nMerged\\nWeightPretrained\\nWeight\\nPretrained\\nWeight\\nAdaptFrozen\\nTrainable\\nMagnitude\\nDirection DirectionDecompose\\n(Initialize)MergeFigure 1. An overview of our proposed DoRA, which decomposes\\nthe pre-trained weight into magnitude anddirection components\\nfor fine-tuning, especially with LoRA to efficiently update the\\ndirection component. Note that || · || cdenotes the vector-wise\\nnorm of a matrix across each column vector.\\nin scale, the expense associated with fine-tuning the entire\\nmodel becomes prohibitively large.\\nTo address this issue, parameter-efficient fine-tuning (PEFT)\\nmethods (Houlsby et al., 2019) have been introduced to fine-\\ntune the pre-trained models with only a minimal number of\\nparameters. Among these, LoRA (Hu et al., 2022), which\\ndoes not change the model architecture, has become notably\\npopular for its simplicity and efficacy. Nevertheless, there\\nis still a capacity gap between LoRA and FT, which is often\\nattributed to the limited number of trainable parameters\\nwithout further exploration of other underlying causes (Hu\\net al., 2022; Kopiczko et al., 2024).\\nDrawing on Weight Normalization (Salimans & Kingma,\\n2016), which achieves faster convergence via improving the\\nconditioning of the gradient with weight reparameterization,\\nwe introduce a novel weight decomposition analysis that ini-\\ntially reparameterizes model weights into magnitude and di-\\nrectional components, subsequently examining the changes\\nin magnitude and direction introduced by LoRA and FT.\\nOur analysis reveals that LoRA and FT exhibit markedly\\n1arXiv:2402.09353v1  [cs.CL]  14 Feb 2024'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to see what the text looks like\n",
    "documents[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4801e74a-8c52-45c4-967d-7a1a94f54ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chunk, Encode, and Store into a Vector Store.\n",
    "\n",
    "To streamline the process, we can make use of the IngestionPipeline\n",
    "class that will apply your specified transformations to the\n",
    "Document's.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations = [\n",
    "        SentenceSplitter(),\n",
    "        OpenAIEmbedding(),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "_nodes = pipeline.run(documents=documents, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02afea25-098b-49c7-a965-21c7576757af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see the nodes\n",
    "# len(_nodes)\n",
    "# _nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44cd8a86-089d-4329-9484-35b98b3a26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a llama-index... wait for it... Index.\n",
    "\n",
    "After uploading your encoded documents into your vector\n",
    "store of choice, you can connect to it with a VectorStoreIndex\n",
    "which then gives you access to all of the llama-index functionality.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b1827-7547-49c6-aba3-82f08d6d86b8",
   "metadata": {},
   "source": [
    "## 2. Retrieve Against A Query\n",
    "\n",
    "![Step2 Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/step2.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49f86af1-db08-4641-89ad-d60abd04e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve relevant documents against a query.\n",
    "\n",
    "With our Index ready, we can now query it to\n",
    "retrieve the most relevant document chunks.\n",
    "\"\"\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "retrieved_nodes = retriever.retrieve(\"What is DoRA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05f9ce3b-a4e3-4862-b58c-2d9fba1f9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view the retrieved node\n",
    "# print(retrieved_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ae2c5-8c2a-41c7-a2eb-85a5562f2db5",
   "metadata": {},
   "source": [
    "## 3. Generate Final Response\n",
    "\n",
    "![Step3 Image](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/step3.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef33c349-eed4-4e35-9b5d-9473adf2ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Context-Augemented Generation.\n",
    "\n",
    "With our Index ready, we can create a QueryEngine\n",
    "that handles the retrieval and context augmentation\n",
    "in order to get the final response.\n",
    "\"\"\"\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4139c48a-ece8-4244-b4eb-7cff74cb1325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# to inspect the default prompt being used\n",
    "print(query_engine.\n",
    "      get_prompts()[\"response_synthesizer:text_qa_template\"]\n",
    "      .default_template.template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6179639d-af96-4a09-b440-b47ad599a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoRA is a method that enhances the learning capacity of LoRA by introducing incremental directional updates that can be adapted by different LoRA variants. It can replace the concept of incremental directional updates with alternative LoRA variants, such as VeRA, which suggests freezing a unique pair of random low-rank matrices shared across all layers and using minimal layer-specific trainable scaling vectors to capture each layer's incremental updates. This approach significantly reduces the number of trainable parameters while maintaining accuracy. DoRA has been shown to consistently outperform LoRA across various rank settings for commonsense reasoning tasks, indicating its effectiveness in improving accuracy with fewer trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is DoRA?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae63946-be38-4807-af2a-8113661a806b",
   "metadata": {},
   "source": [
    "## In Summary\n",
    "\n",
    "- LLMs as powerful as they are, don't perform too well with knowledge-intensive tasks (domain specific, updated data, long-tail)\n",
    "- Context augmentation has been shown (in a few studies) to outperform LLMs without augmentation\n",
    "- In this notebook, we showed one such example that follows that pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc857227-3fed-4bb6-a062-99ea3c55e294",
   "metadata": {},
   "source": [
    "# LlamaIndex Has More To Offer\n",
    "\n",
    "- Data infrastructure that enables production-grade, advanced RAG systems\n",
    "- Agentic solutions\n",
    "- Newly released: `llama-index-networks`\n",
    "- Enterprise offerings (alpha):\n",
    "    - LlamaParse (proprietary complex PDF parser) and\n",
    "    - LlamaCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1c027-be8b-48f4-87ee-06f3e2c71797",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "[website](https://www.llamaindex.ai/) ◦ [llamahub](https://llamahub.ai) ◦ [github](https://github.com/run-llama/llama_index) ◦ [medium](https://medium.com/@llama_index) ◦ [rag-bootcamp-poster](https://d3ddy8balm3goa.cloudfront.net/rag-bootcamp-vector/final_poster.excalidraw.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876f95e-0a1c-4fcf-b155-c8ed688f6649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-bootcamp",
   "language": "python",
   "name": "rag-bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
